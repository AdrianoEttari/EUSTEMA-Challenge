{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a style = background:lightgreen;color:black>Settlement Prediction on train_s (modeling) </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../DATA/final_train_s_dummies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[['Tax Related', 'Number of Lawyers',\n",
    "     'Number of Legal Parties', 'Value formatted',\n",
    "       'Unified Contribution formatted', 'Milano', 'Bari', 'Bologna', 'Genova',\n",
    "       'Palermo', 'Napoli', 'Torino', 'Trento', 'Roma', \"L'Aquila\", 'Potenza',\n",
    "       'Perugia', 'Campobasso', 'Firenze', 'Cagliari', 'Venezia', 'Cosenza',\n",
    "       'Ancona', 'Trieste', 'Aosta','OR-140999', 'OR-145009', 'OR-139999',\n",
    "       'OR-145999', 'OR-130099', 'OR-101003', 'OR-130121', 'OR-130111',\n",
    "       'OR-130131', 'OR-101002', 'OR-180002', 'OSA-180002']]\n",
    "y=data['Settlement']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a style=background:yellow;color:black> Standardization and Split in training, validation and testing set </a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to scale just the variables that are neither dummies nor boolean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_to_scale = X[['Number of Lawyers','Number of Legal Parties', 'Value formatted',\n",
    "       'Unified Contribution formatted']]\n",
    "X_not_to_scale = X[['Tax Related','Milano', 'Bari', 'Bologna', 'Genova',\n",
    "       'Palermo', 'Napoli', 'Torino', 'Trento', 'Roma', \"L'Aquila\", 'Potenza',\n",
    "       'Perugia', 'Campobasso', 'Firenze', 'Cagliari', 'Venezia', 'Cosenza',\n",
    "       'Ancona', 'Trieste', 'Aosta','OR-140999', 'OR-145009', 'OR-139999',\n",
    "       'OR-145999', 'OR-130099', 'OR-101003', 'OR-130121', 'OR-130111',\n",
    "       'OR-130131', 'OR-101002', 'OR-180002', 'OSA-180002']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = pd.DataFrame(y)\n",
    "std_scale = StandardScaler()\n",
    "X_scaled = std_scale.fit_transform(X_to_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled_df = pd.DataFrame(X_scaled, columns=[X_to_scale.columns])\n",
    "X_scaled_df = pd.concat([X_scaled_df, X_not_to_scale], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_var_names = []\n",
    "for var_name in X_scaled_df.columns:\n",
    "    if '(' not in var_name:\n",
    "        new_var_names.append(var_name)\n",
    "    else:\n",
    "        new_var_names.append(var_name.replace('(', '').replace(')', '')\n",
    "        .replace('\\'','').replace(',','').replace('\"',''))\n",
    "new_var_names = []\n",
    "for var_name in X_scaled_df.columns:\n",
    "    if type(var_name) != tuple:\n",
    "        new_var_names.append(var_name)\n",
    "    else:\n",
    "        new_var_names.append(var_name[0])\n",
    "\n",
    "X_scaled_df = pd.DataFrame(X_scaled_df.values, columns=new_var_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X_scaled_df, y, random_state=0,\n",
    "                                                            test_size=0.1)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val,random_state=0,\n",
    "                                                            test_size=0.2  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4575ad0",
   "metadata": {
    "id": "f4575ad0"
   },
   "source": [
    "## <a style=background:yellow;color:black id='reg_tree'> REGRESSION-TREE</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb402f2f",
   "metadata": {
    "id": "bb402f2f"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34018f77",
   "metadata": {
    "id": "34018f77",
    "outputId": "03eb7765-8fe8-4de1-e106-7fa686b3e810"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeRegressor(criterion=&#x27;absolute_error&#x27;, max_features=&#x27;sqrt&#x27;,\n",
       "                      min_samples_split=10, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeRegressor</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeRegressor(criterion=&#x27;absolute_error&#x27;, max_features=&#x27;sqrt&#x27;,\n",
       "                      min_samples_split=10, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeRegressor(criterion='absolute_error', max_features='sqrt',\n",
       "                      min_samples_split=10, random_state=42)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_tree = DecisionTreeRegressor(random_state=42, criterion='absolute_error',\n",
    "                                min_samples_split=10,\n",
    "                                max_features='sqrt')\n",
    "\n",
    "reg_tree.fit(X_train_val,y_train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c34a58d9",
   "metadata": {
    "id": "c34a58d9",
    "outputId": "537f811c-1583-40ee-f291-9ffabc166651"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_true</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>residuals</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>971.67</td>\n",
       "      <td>600.00</td>\n",
       "      <td>-371.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>409.63</td>\n",
       "      <td>464.00</td>\n",
       "      <td>54.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7290.49</td>\n",
       "      <td>11659.89</td>\n",
       "      <td>4369.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.00</td>\n",
       "      <td>3913.24</td>\n",
       "      <td>3912.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1082.64</td>\n",
       "      <td>1777.23</td>\n",
       "      <td>694.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9655</th>\n",
       "      <td>752.05</td>\n",
       "      <td>600.00</td>\n",
       "      <td>-152.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9656</th>\n",
       "      <td>314.39</td>\n",
       "      <td>317.00</td>\n",
       "      <td>2.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9657</th>\n",
       "      <td>500.00</td>\n",
       "      <td>1052.00</td>\n",
       "      <td>552.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9658</th>\n",
       "      <td>135.43</td>\n",
       "      <td>750.00</td>\n",
       "      <td>614.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9659</th>\n",
       "      <td>1946.43</td>\n",
       "      <td>2633.39</td>\n",
       "      <td>686.96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9660 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       y_true    y_pred  residuals\n",
       "0      971.67    600.00    -371.67\n",
       "1      409.63    464.00      54.37\n",
       "2     7290.49  11659.89    4369.40\n",
       "3        1.00   3913.24    3912.24\n",
       "4     1082.64   1777.23     694.59\n",
       "...       ...       ...        ...\n",
       "9655   752.05    600.00    -152.05\n",
       "9656   314.39    317.00       2.61\n",
       "9657   500.00   1052.00     552.00\n",
       "9658   135.43    750.00     614.57\n",
       "9659  1946.43   2633.39     686.96\n",
       "\n",
       "[9660 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "y_pred = reg_tree.predict(X_test)\n",
    "residuals = np.ravel(np.array(y_pred)) - np.ravel(np.array(y_test))\n",
    "\n",
    "pd.DataFrame({'y_true':np.ravel(y_test), 'y_pred':np.ravel(y_pred), 'residuals':np.ravel(residuals)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "867.1266563146997\n"
     ]
    }
   ],
   "source": [
    "print(mean_absolute_error(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a style=background:yellow;color:black> MARS (Check the R Script)</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* One major problem with trees is their high variance. Often a small change in the data can result in a very different series of splits. This is due to the hierarchical nature of the process (if an error occurs in the first split, then all the other splits can just make it worse and worse). A way to reduce this variance is to use Bagging (We've done it with Random Forest).\n",
    "* The lack of smoothness of the predictions surfaces and the difficulty in capturing additive structure can be a problem for regression tasks. We can solve this problems by using <a style=color:deepskyblue> **MARS** </a> (Multivariate Additive Regression Splines)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This chunk is for the MARS model.\n",
    "# final_train_s_dummies_std = pd.concat([X_test, X_train_val],axis=0 )\n",
    "# final_train_s_dummies_std['Settlement'] = pd.concat([y_test, y_train_val], axis = 0)\n",
    "# final_train_s_dummies_std.to_excel('final_train_s_dummies_std.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12418d4",
   "metadata": {
    "id": "a12418d4"
   },
   "source": [
    "## <a style=background:yellow;color:black id='Random_Forest'> Random Forest </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a7c7903",
   "metadata": {
    "id": "8a7c7903"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rnd_reg = RandomForestRegressor(n_estimators=50,\n",
    "                                #n_jobs=5,\n",
    "                                max_depth=10,\n",
    "                                criterion='absolute_error',\n",
    "                               random_state = 42,\n",
    "                               min_samples_split=20,\n",
    "                               min_samples_leaf=20,\n",
    "                               max_leaf_nodes=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17811dbe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "17811dbe",
    "outputId": "6a83d955-a72a-4b41-b6e3-08f8ccd952cb"
   },
   "outputs": [],
   "source": [
    "rnd_reg.fit(X_train_val, np.array(y_train_val).ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590fa7b8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "590fa7b8",
    "outputId": "009606d8-1c45-4e12-9a22-8ff621af75e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "765.0234303933746\n",
      "727.9669157159126\n"
     ]
    }
   ],
   "source": [
    "y_pred = rnd_reg.predict(X_test)\n",
    "\n",
    "residuals = np.array(y_pred) - np.array(y_test)\n",
    "\n",
    "print(mean_absolute_error(y_test, y_pred))\n",
    "print(mean_absolute_error(y_train_val, rnd_reg.predict(X_train_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "# joblib.dumb(rnd_red, '../models/rnd_reg_50_estimators.sav')\n",
    "# rnd_reg = joblib.load('../models/rnd_reg_50_estimators.sav')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ea9f5e7",
   "metadata": {},
   "source": [
    "Let's destandardize X_train_val and X_test (the reason why we do it is that we want to build a dataset (data_for_plotting.csv) in order to see the results of the models in Tableau considering the original scale of the variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QL-H5SVLeN_t",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QL-H5SVLeN_t",
    "outputId": "b406ebde-6899-4e72-8981-58db46b26138"
   },
   "outputs": [],
   "source": [
    "X_train_val_inv_trans = pd.DataFrame(std_scale.inverse_transform(X_train_val[['Number of Lawyers','Number of Legal Parties', 'Value formatted',\n",
    "       'Unified Contribution formatted']]), columns = ['Number of Lawyers','Number of Legal Parties', 'Value formatted',\n",
    "       'Unified Contribution formatted'], index = X_train_val.index)\n",
    "\n",
    "X_test_inv_trans = pd.DataFrame(std_scale.inverse_transform(X_test[['Number of Lawyers','Number of Legal Parties', 'Value formatted',\n",
    "       'Unified Contribution formatted']]), columns = ['Number of Lawyers','Number of Legal Parties', 'Value formatted',\n",
    "       'Unified Contribution formatted'], index=X_test.index)\n",
    "\n",
    "X_train_val_not_std = X_train_val[['Tax Related','Milano', 'Bari', 'Bologna', 'Genova',\n",
    "       'Palermo', 'Napoli', 'Torino', 'Trento', 'Roma', \"L'Aquila\", 'Potenza',\n",
    "       'Perugia', 'Campobasso', 'Firenze', 'Cagliari', 'Venezia', 'Cosenza',\n",
    "       'Ancona', 'Trieste', 'Aosta','OR-140999', 'OR-145009', 'OR-139999',\n",
    "       'OR-145999', 'OR-130099', 'OR-101003', 'OR-130121', 'OR-130111',\n",
    "       'OR-130131', 'OR-101002', 'OR-180002', 'OSA-180002']]\n",
    "\n",
    "X_test_not_std = X_test[['Tax Related','Milano', 'Bari', 'Bologna', 'Genova',\n",
    "       'Palermo', 'Napoli', 'Torino', 'Trento', 'Roma', \"L'Aquila\", 'Potenza',\n",
    "       'Perugia', 'Campobasso', 'Firenze', 'Cagliari', 'Venezia', 'Cosenza',\n",
    "       'Ancona', 'Trieste', 'Aosta','OR-140999', 'OR-145009', 'OR-139999',\n",
    "       'OR-145999', 'OR-130099', 'OR-101003', 'OR-130121', 'OR-130111',\n",
    "       'OR-130131', 'OR-101002', 'OR-180002', 'OSA-180002']]\n",
    "\n",
    "X_train_val_destandardized = pd.concat([X_train_val_inv_trans, X_train_val_not_std],axis=1)\n",
    "X_test_destandardized = pd.concat([X_test_inv_trans, X_test_not_std],axis=1)\n",
    "\n",
    "X_train_val_destandardized = pd.DataFrame(np.array(X_train_val_destandardized), columns=[['Number of Lawyers',\n",
    "     'Number of Legal Parties', 'Value formatted',\n",
    "       'Unified Contribution formatted', 'Tax Related','Milano', 'Bari', 'Bologna', 'Genova',\n",
    "       'Palermo', 'Napoli', 'Torino', 'Trento', 'Roma', \"L'Aquila\", 'Potenza',\n",
    "       'Perugia', 'Campobasso', 'Firenze', 'Cagliari', 'Venezia', 'Cosenza',\n",
    "       'Ancona', 'Trieste', 'Aosta','OR-140999', 'OR-145009', 'OR-139999',\n",
    "       'OR-145999', 'OR-130099', 'OR-101003', 'OR-130121', 'OR-130111',\n",
    "       'OR-130131', 'OR-101002', 'OR-180002', 'OSA-180002']], index = X_train_val_destandardized.index)\n",
    "\n",
    "X_test_destandardized = pd.DataFrame(np.array(X_test_destandardized), columns=[['Number of Lawyers',\n",
    "     'Number of Legal Parties', 'Value formatted',\n",
    "       'Unified Contribution formatted', 'Tax Related','Milano', 'Bari', 'Bologna', 'Genova',\n",
    "       'Palermo', 'Napoli', 'Torino', 'Trento', 'Roma', \"L'Aquila\", 'Potenza',\n",
    "       'Perugia', 'Campobasso', 'Firenze', 'Cagliari', 'Venezia', 'Cosenza',\n",
    "       'Ancona', 'Trieste', 'Aosta','OR-140999', 'OR-145009', 'OR-139999',\n",
    "       'OR-145999', 'OR-130099', 'OR-101003', 'OR-130121', 'OR-130111',\n",
    "       'OR-130131', 'OR-101002', 'OR-180002', 'OSA-180002']], index = X_test_destandardized.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a598fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_MARS = np.array(pd.read_excel('y_pred.xlsx')).ravel()\n",
    "\n",
    "# data_for_plotting = pd.DataFrame({'y_true':np.concatenate([np.array(y_test),np.array(y_train_val)]).ravel(),\n",
    "#  'y_pred_rnd_for_no_grid':np.concatenate([np.array(rnd_reg.predict(X_test)),np.array(rnd_reg.predict(X_train_val))]),\n",
    "# 'y_pred_MARS':y_pred_MARS,\n",
    "#  'Value formatted':np.array(pd.concat([X_test_destandardized['Value formatted'], X_train_val_destandardized['Value formatted']], axis=0)).ravel(),\n",
    "#  'Unified contribution formatted':np.array(pd.concat([X_test_destandardized['Unified Contribution formatted'], X_train_val_destandardized['Unified Contribution formatted']], axis=0)).ravel(),\n",
    "# 'train_test':np.concatenate([np.repeat('test',y_test.shape[0]),np.repeat('train',y_train_val.shape[0])])})\n",
    "\n",
    "# data_for_plotting.to_csv('data_for_plotting.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe76ad9d",
   "metadata": {
    "id": "fe76ad9d"
   },
   "source": [
    "## <a style=background:yellow;color:black id='xgboost'> XGBoost</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'booster': 'gbtree',\n",
    "    'learning_rate': 0.05,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'mae',\n",
    "    'eta': 0.01,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.5,\n",
    "    'tree_method': 'exact',\n",
    "    'verbosity': 0,\n",
    "    'max_depth': 10,\n",
    "}\n",
    "\n",
    "# Define the dataset for xgboost\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dval = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "# # Define the learning rate schedule\n",
    "# def learning_rate_schedule(iteration):\n",
    "#     if iteration < 100:\n",
    "#         return 0.1\n",
    "#     elif 100 <= iteration < 200:\n",
    "#         return 0.01\n",
    "#     else:\n",
    "#         return 0.001\n",
    "\n",
    "# # Define the callback to update the learning rate\n",
    "# def update_learning_rate(env):\n",
    "#     lr = learning_rate_schedule(env.iteration)\n",
    "#     env.model.params['learning_rate'] = lr\n",
    "\n",
    "# Define the stopping criteria\n",
    "early_stopping_rounds = 50\n",
    "\n",
    "# Train the model with early stopping\n",
    "# model = xgb.train(params, dtrain, num_boost_round=500, evals=[(dval, 'val')], early_stopping_rounds=early_stopping_rounds, verbose_eval=False, callbacks=[update_learning_rate])\n",
    "model = xgb.train(params, dtrain, num_boost_round=1000, evals=[(dval, 'val')], early_stopping_rounds=early_stopping_rounds, verbose_eval=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_model(\"../models/xgb_train_s.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "805.6419785217511\n"
     ]
    }
   ],
   "source": [
    "# xgb_model = xgb.Booster()\n",
    "# xgb_model.load_model('../models/xgb_train_s.model')\n",
    "\n",
    "dtest = xgb.DMatrix(X_test)\n",
    "y_pred_test = model.predict(dtest)\n",
    "print(mean_absolute_error(y_pred_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_for_plotting = pd.read_csv('../DATA/data_for_plotting.csv')\n",
    "# dtrain_val = xgb.DMatrix(X_train_val)\n",
    "# data_for_plotting['y_pred_xgb'] = np.concatenate([np.array(model.predict(dtest)),np.array(model.predict(dtrain_val))])\n",
    "# data_for_plotting.to_csv('../DATA/data_for_plotting.csv')\n",
    "# data_for_plotting = pd.read_csv('../DATA/data_for_plotting.csv')\n",
    "# data_for_plotting.to_excel('../DATA/data_for_plotting.xlsx')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37bce5c1",
   "metadata": {},
   "source": [
    "## <a style=background:yellow;color:black id='neural_network'> Neural Network </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91b150b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following chunk is for reproducibility purposes. It is not necessary to run it. Moreover, keras.backend.clear_session() is useful in cases where you want to start a new session with a fresh set of variables, or if you want to clear the session to free up memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "823022a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "keras.backend.clear_session() \n",
    "np.random.seed(42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following chunk we have the learning rate scheduler of the neural network (for the first 3 epochs it is equal to the lr defined in the .compile() method and afterwords at each epoch it decreases of lr*exp(-0.025)). Moreover there is an early stopping callback and a tensorboard callback. The tensorboard callback is used to visualize the training process in Tensorboard. The early stopping callback is used to stop the training process when the validation loss doesn't decrease for 10 epochs. The get_run_logdir function is used to create a unique subdirectory in which to save the Tensorboard log files for each training run. This is necessary to avoid problems when you launch multiple Tensorboard instances at the same time.\n",
    "\n",
    "The histogram_freq parameter in the TensorBoard callback of Keras determines how often (in number of epochs) to compute activation and weight histograms for the layers of the model. If set to 1, histograms will be computed after every epoch. If set to 0, histograms will not be computed. If set to a value greater than 1, histograms will be computed every histogram_freq epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e616b741",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch < 3:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.025)\n",
    "\n",
    "def get_run_logdir(root_logdir, model_name):\n",
    "    import time\n",
    "    run_id = time.strftime('run_%Y_%m_%d-%H_%M_%S')\n",
    "    return os.path.join(root_logdir, model_name+'_'+run_id)\n",
    "\n",
    "model_name = input('Enter model name: ')\n",
    "root_logdir = os.path.join(os.curdir,'..', 'my_logs')\n",
    "run_logdir = get_run_logdir(root_logdir, model_name)\n",
    "\n",
    "lr_schedule_cb = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "early_stop_cb = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir,\n",
    "                                            histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dc5342",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(64, activation=\"relu\", kernel_initializer=\"he_normal\",input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(128, activation='relu', kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dense(64, activation='relu', kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss='mae', optimizer=keras.optimizers.Adam(learning_rate=0.015))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae17809",
   "metadata": {
    "id": "eae17809"
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val),callbacks=[tensorboard_cb, lr_schedule_cb, early_stop_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('../models/NeuralNet_train_s.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ecaf48",
   "metadata": {
    "id": "73ecaf48"
   },
   "outputs": [],
   "source": [
    "!tensorboard --logdir=../my_logs --port=6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77ff69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3ba329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_for_plotting = pd.read_excel('data_for_plotting.xlsx')\n",
    "# data_for_plotting['y_pred_neural_network_oversamp'] = np.concatenate([np.array(model.predict(X_test)),np.array(model.predict(X_train_val))])\n",
    "# data_for_plotting.to_excel('data_for_plotting.xlsx')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a style=background:yellow;color:black> Downsampling and Upsampling </a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter we are going to try to solve the unbalance problem with downsampling and upsampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../DATA/final_train_s_dummies.csv')\n",
    "\n",
    "X_y = data[['Tax Related', 'Number of Lawyers',\n",
    "     'Number of Legal Parties', 'Value formatted',\n",
    "       'Unified Contribution formatted', 'Milano', 'Bari', 'Bologna', 'Genova',\n",
    "       'Palermo', 'Napoli', 'Torino', 'Trento', 'Roma', \"L'Aquila\", 'Potenza',\n",
    "       'Perugia', 'Campobasso', 'Firenze', 'Cagliari', 'Venezia', 'Cosenza',\n",
    "       'Ancona', 'Trieste', 'Aosta','OR-140999', 'OR-145009', 'OR-139999',\n",
    "       'OR-145999', 'OR-130099', 'OR-101003', 'OR-130121', 'OR-130111',\n",
    "       'OR-130131', 'OR-101002', 'OR-180002', 'OSA-180002', 'Settlement']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This chunk takes about 20 seconds to run\n",
    "ranges_value = []\n",
    "for i in range(X_y.shape[0]):\n",
    "    if (X_y.iloc[i]['Value formatted'] >= 0) & (X_y.iloc[i]['Value formatted'] < 1) :\n",
    "         ranges_value.append('[0-1)')\n",
    "    elif (X_y.iloc[i]['Value formatted'] >= 1) & (X_y.iloc[i]['Value formatted'] <= 1000) :\n",
    "         ranges_value.append('[1,1000]')\n",
    "    elif (X_y.iloc[i]['Value formatted'] > 1000) & (X_y.iloc[i]['Value formatted'] <= 1100) :\n",
    "         ranges_value.append('(1000,1100]')\n",
    "    elif (X_y.iloc[i]['Value formatted'] > 1100) & (X_y.iloc[i]['Value formatted'] <= 5200) :\n",
    "         ranges_value.append('(1100,5200]')\n",
    "    elif (X_y.iloc[i]['Value formatted'] > 5200) & (X_y.iloc[i]['Value formatted'] <= 26000) :\n",
    "         ranges_value.append('(5200,26000]')\n",
    "    elif (X_y.iloc[i]['Value formatted'] > 26000) & (X_y.iloc[i]['Value formatted'] <= 52000) :\n",
    "         ranges_value.append('(26000,52000]')\n",
    "    elif (X_y.iloc[i]['Value formatted'] > 52000) & (X_y.iloc[i]['Value formatted'] <= 260000) :\n",
    "         ranges_value.append('(52000,260000]')\n",
    "    elif (X_y.iloc[i]['Value formatted'] > 260000) & (X_y.iloc[i]['Value formatted'] <= 520000) :\n",
    "         ranges_value.append('(260000,520000]')\n",
    "    elif (X_y.iloc[i]['Value formatted'] > 520000):\n",
    "         ranges_value.append('(520000,inf)')\n",
    "    else:\n",
    "        ranges_value.append('error')\n",
    "\n",
    "X_y['ranges_value']=ranges_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ranges_value\n",
       "(1000,1100]        5369\n",
       "(1100,5200]       48556\n",
       "(26000,52000]        61\n",
       "(5200,26000]       9539\n",
       "(52000,260000]        8\n",
       "(520000,inf)          2\n",
       "[0-1)               748\n",
       "[1,1000]          32308\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_y.groupby(['ranges_value']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j3/6zzd1jr16pb3yxx70_376pmw0000gn/T/ipykernel_6407/3290639181.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_y.drop(['Settlement'], axis=1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "y = X_y['Settlement']\n",
    "\n",
    "X_y.drop(['Settlement'], axis=1, inplace=True)\n",
    "\n",
    "X_to_scale = X_y[['Number of Lawyers','Number of Legal Parties', 'Value formatted',\n",
    "       'Unified Contribution formatted']]\n",
    "\n",
    "X_not_to_scale = X_y[['Tax Related','Milano', 'Bari', 'Bologna', 'Genova',\n",
    "       'Palermo', 'Napoli', 'Torino', 'Trento', 'Roma', \"L'Aquila\", 'Potenza',\n",
    "       'Perugia', 'Campobasso', 'Firenze', 'Cagliari', 'Venezia', 'Cosenza',\n",
    "       'Ancona', 'Trieste', 'Aosta','OR-140999', 'OR-145009', 'OR-139999',\n",
    "       'OR-145999', 'OR-130099', 'OR-101003', 'OR-130121', 'OR-130111',\n",
    "       'OR-130131', 'OR-101002', 'OR-180002', 'OSA-180002', 'ranges_value']]\n",
    "\n",
    "\n",
    "y = pd.DataFrame(y)\n",
    "std_scale = StandardScaler()\n",
    "X_scaled = std_scale.fit_transform(X_to_scale)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X_to_scale.columns)\n",
    "X_scaled_df = pd.concat([X_scaled_df, X_not_to_scale], axis=1)\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X_scaled_df, y, random_state=0,\n",
    "                                                            test_size=0.1)\n",
    "X_test.drop(['ranges_value'], axis=1, inplace=True)\n",
    "X_y_train_val = pd.concat([X_train_val,y_train_val],axis=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following chunk we're going to:\n",
    "\n",
    "*  build a downsampled dataset (the four classes with the highest number of observations will be downsampled to 4831 that is the number of observations of the class with less observations among them);\n",
    "* build an upsampled dataset (the (5200, 26000] class will be upsampled to 29000 observations). The reason is that for this class we have the highest MAE considering the number of observations per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_df = pd.DataFrame(columns=X_y_train_val.columns)\n",
    "upsampled_df = pd.DataFrame(columns=X_y_train_val.columns)\n",
    "\n",
    "for i in X_y_train_val['ranges_value'].unique():\n",
    "    if i in ['[1,1000]','(1000,1100]','(1100,5200]']:\n",
    "        downsampled_df = downsampled_df.append(X_y_train_val[X_y_train_val['ranges_value']==i].sample(4831, replace=True))\n",
    "        upsampled_df = upsampled_df.append(X_y_train_val[X_y_train_val['ranges_value']==i])\n",
    "    elif i == '(5200,26000]':\n",
    "        downsampled_df = downsampled_df.append(X_y_train_val[X_y_train_val['ranges_value']==i].sample(4831, replace=True))\n",
    "        upsampled_df = upsampled_df.append(X_y_train_val[X_y_train_val['ranges_value']==i].sample(29000, replace=True))\n",
    "    else:\n",
    "        downsampled_df = downsampled_df.append(X_y_train_val[X_y_train_val['ranges_value']==i])\n",
    "        upsampled_df = upsampled_df.append(X_y_train_val[X_y_train_val['ranges_value']==i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following chunk we have the learning rate scheduler of the neural network (for the first 3 epochs it is equal to the lr defined in the .compile and afterwords at each epoch it decreases of lr*exp(-0.025)). Moreover there is an early stopping callback and a tensorboard callback. The tensorboard callback is used to visualize the training process in Tensorboard. The early stopping callback is used to stop the training process when the validation loss doesn't decrease for 10 epochs. The get_run_logdir function is used to create a unique subdirectory in which to save the Tensorboard log files for each training run. This is necessary to avoid problems when you launch multiple Tensorboard instances at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_convert = ['Tax Related', 'Milano', 'Bari',\n",
    "       'Bologna', 'Genova', 'Palermo', 'Napoli', 'Torino', 'Trento', 'Roma',\n",
    "       \"L'Aquila\", 'Potenza', 'Perugia', 'Campobasso', 'Firenze', 'Cagliari',\n",
    "       'Venezia', 'Cosenza', 'Ancona', 'Trieste', 'Aosta', 'OR-140999',\n",
    "       'OR-145009', 'OR-139999', 'OR-145999', 'OR-130099', 'OR-101003',\n",
    "       'OR-130121', 'OR-130111', 'OR-130131', 'OR-101002', 'OR-180002',\n",
    "       'OSA-180002']\n",
    "\n",
    "for column in columns_to_convert:\n",
    "    downsampled_df[column] = downsampled_df[column].astype('int64')\n",
    "    upsampled_df[column] = upsampled_df[column].astype('int64')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following chunk builds and fit a neural network on the downsampled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e616b741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Set random seed and clear the session\n",
    "tf.random.set_seed(42)\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# Callbacks\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 3:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.025)\n",
    "\n",
    "def get_run_logdir(root_logdir, model_name):\n",
    "    import time\n",
    "    run_id = time.strftime('run_%Y_%m_%d-%H_%M_%S')\n",
    "    return os.path.join(root_logdir, model_name+'_'+run_id)\n",
    "\n",
    "model_name = input('Enter model name: ')\n",
    "root_logdir = os.path.join(os.curdir,'..', 'my_logs')\n",
    "run_logdir = get_run_logdir(root_logdir, model_name)\n",
    "\n",
    "lr_schedule_cb = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "early_stop_cb = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir,\n",
    "                                            histogram_freq=1)\n",
    "\n",
    "# Model design\n",
    "model1 = keras.models.Sequential([\n",
    "    keras.layers.Dense(64, activation=\"relu\", kernel_initializer=\"he_normal\",input_shape=(37,)),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(128, activation='relu', kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(64, activation='relu', kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model1.compile(loss='mae', optimizer=keras.optimizers.Adam(learning_rate=0.015))\n",
    "\n",
    "# Split in train and validation set\n",
    "train_downsampled, val_downsampled = train_test_split(downsampled_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fitting\n",
    "history = model1.fit(train_downsampled.drop(columns=['Settlement','ranges_value']),\n",
    "                     train_downsampled['Settlement'],\n",
    "                      epochs=100,\n",
    "                       validation_data=(val_downsampled.drop(columns=['Settlement','ranges_value']),\n",
    "                        val_downsampled['Settlement']),\n",
    "                       callbacks=[tensorboard_cb, lr_schedule_cb, early_stop_cb])\n",
    "\n",
    "# MAE on the test set\n",
    "mean_absolute_error(y_test, model1.predict(X_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following chunk builds and fit a neural network on the upsampled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae17809",
   "metadata": {
    "id": "eae17809"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Set random seed and clear the session\n",
    "tf.random.set_seed(42)\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# Callbacks\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 3:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.025)\n",
    "\n",
    "def get_run_logdir(root_logdir, model_name):\n",
    "    import time\n",
    "    run_id = time.strftime('run_%Y_%m_%d-%H_%M_%S')\n",
    "    return os.path.join(root_logdir, model_name+'_'+run_id)\n",
    "\n",
    "model_name = input('Enter model name: ')\n",
    "root_logdir = os.path.join(os.curdir,'..', 'my_logs')\n",
    "run_logdir = get_run_logdir(root_logdir, model_name)\n",
    "\n",
    "lr_schedule_cb = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "early_stop_cb = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir,\n",
    "                                            histogram_freq=1)\n",
    "\n",
    "# Model design\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(64, activation=\"relu\", kernel_initializer=\"he_normal\",input_shape=(37,)),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(128, activation='relu', kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(64, activation='relu', kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss='mae', optimizer=keras.optimizers.Adam(learning_rate=0.015))\n",
    "\n",
    "# Split in train and validation set\n",
    "train_downsampled, val_downsampled = train_test_split(upsampled_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fitting\n",
    "history = model.fit(train_downsampled.drop(columns=['Settlement','ranges_value']),\n",
    "                     train_downsampled['Settlement'],\n",
    "                      epochs=100,\n",
    "                       validation_data=(val_downsampled.drop(columns=['Settlement','ranges_value']),\n",
    "                        val_downsampled['Settlement']),\n",
    "                       callbacks=[tensorboard_cb, lr_schedule_cb, early_stop_cb])\n",
    "\n",
    "# MAE on the test set\n",
    "mean_absolute_error(y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ecaf48",
   "metadata": {
    "id": "73ecaf48"
   },
   "outputs": [],
   "source": [
    "# !tensorboard --logdir=../my_logs --port=6006"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following chunk is to fill data_for_plotting.csv with the prediction of the upsampling neural network and of the downsampling neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77ff69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_for_plotting = pd.read_csv('../DATA/data_for_plotting.csv')\n",
    "# data = pd.read_csv('../DATA/final_train_s_dummies.csv')\n",
    "\n",
    "# X = data[['Tax Related', 'Number of Lawyers',\n",
    "#      'Number of Legal Parties', 'Value formatted',\n",
    "#        'Unified Contribution formatted', 'Milano', 'Bari', 'Bologna', 'Genova',\n",
    "#        'Palermo', 'Napoli', 'Torino', 'Trento', 'Roma', \"L'Aquila\", 'Potenza',\n",
    "#        'Perugia', 'Campobasso', 'Firenze', 'Cagliari', 'Venezia', 'Cosenza',\n",
    "#        'Ancona', 'Trieste', 'Aosta','OR-140999', 'OR-145009', 'OR-139999',\n",
    "#        'OR-145999', 'OR-130099', 'OR-101003', 'OR-130121', 'OR-130111',\n",
    "#        'OR-130131', 'OR-101002', 'OR-180002', 'OSA-180002']]\n",
    "\n",
    "# y=data['Settlement']\n",
    "\n",
    "# X_to_scale = X[['Number of Lawyers','Number of Legal Parties', 'Value formatted',\n",
    "#        'Unified Contribution formatted']]\n",
    "\n",
    "# X_not_to_scale = X[['Tax Related','Milano', 'Bari', 'Bologna', 'Genova',\n",
    "#        'Palermo', 'Napoli', 'Torino', 'Trento', 'Roma', \"L'Aquila\", 'Potenza',\n",
    "#        'Perugia', 'Campobasso', 'Firenze', 'Cagliari', 'Venezia', 'Cosenza',\n",
    "#        'Ancona', 'Trieste', 'Aosta','OR-140999', 'OR-145009', 'OR-139999',\n",
    "#        'OR-145999', 'OR-130099', 'OR-101003', 'OR-130121', 'OR-130111',\n",
    "#        'OR-130131', 'OR-101002', 'OR-180002', 'OSA-180002']]\n",
    "       \n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# y = pd.DataFrame(y)\n",
    "# std_scale = StandardScaler()\n",
    "# X_scaled = std_scale.fit_transform(X_to_scale)\n",
    "# X_scaled_df = pd.DataFrame(X_scaled, columns=[X_to_scale.columns])\n",
    "# X_scaled_df = pd.concat([X_scaled_df, X_not_to_scale], axis=1)\n",
    "# X_train_val, X_test, y_train_val, y_test = train_test_split(X_scaled_df, y, random_state=0,\n",
    "#                                                             test_size=0.1)\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val,random_state=0,\n",
    "#                                                             test_size=0.2  )\n",
    "\n",
    "# X_train_val = pd.DataFrame(np.array(X_train_val), columns=[['Number of Lawyers',\n",
    "#      'Number of Legal Parties', 'Value formatted',\n",
    "#        'Unified Contribution formatted', 'Tax Related','Milano', 'Bari', 'Bologna', 'Genova',\n",
    "#        'Palermo', 'Napoli', 'Torino', 'Trento', 'Roma', \"L'Aquila\", 'Potenza',\n",
    "#        'Perugia', 'Campobasso', 'Firenze', 'Cagliari', 'Venezia', 'Cosenza',\n",
    "#        'Ancona', 'Trieste', 'Aosta','OR-140999', 'OR-145009', 'OR-139999',\n",
    "#        'OR-145999', 'OR-130099', 'OR-101003', 'OR-130121', 'OR-130111',\n",
    "#        'OR-130131', 'OR-101002', 'OR-180002', 'OSA-180002']], index = X_train_val.index)\n",
    "\n",
    "# X_test = pd.DataFrame(np.array(X_test), columns=[['Number of Lawyers',\n",
    "#      'Number of Legal Parties', 'Value formatted',\n",
    "#        'Unified Contribution formatted', 'Tax Related','Milano', 'Bari', 'Bologna', 'Genova',\n",
    "#        'Palermo', 'Napoli', 'Torino', 'Trento', 'Roma', \"L'Aquila\", 'Potenza',\n",
    "#        'Perugia', 'Campobasso', 'Firenze', 'Cagliari', 'Venezia', 'Cosenza',\n",
    "#        'Ancona', 'Trieste', 'Aosta','OR-140999', 'OR-145009', 'OR-139999',\n",
    "#        'OR-145999', 'OR-130099', 'OR-101003', 'OR-130121', 'OR-130111',\n",
    "#        'OR-130131', 'OR-101002', 'OR-180002', 'OSA-180002']], index = X_test.index)\n",
    "\n",
    "# data_for_plotting['y_pred_neural_network_upsamp'] = np.concatenate([np.array(model.predict(X_test)),np.array(model.predict(X_train_val))])\n",
    "# data_for_plotting['y_pred_neural_network_downsamp'] = np.concatenate([np.array(model1.predict(X_test)),np.array(model1.predict(X_train_val))])\n",
    "\n",
    "# data_for_plotting.to_csv('../DATA/data_for_plotting.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "28d2d24dc05509bf292e34803b47a1e549616d0bedf6bebdafbe5e542b7fb512"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
